{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Info_miners_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qiyUHZ-e0AP6",
        "SM_p06xb0Vo3",
        "0WXAdfJI0fdT",
        "5rGAGvbFE5Q5",
        "QWb90_i7F1HX",
        "X4Pnjn_CL6w_",
        "3Nq-LZxL3svS",
        "f6lzL69V38IS",
        "0O27_R-dS43N",
        "0fiTf_7k4u6-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subha9542/Datamining/blob/master/Info_miners_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiyUHZ-e0AP6"
      },
      "source": [
        "## Importing required libraries and defining Utility functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlYSBSST_uQP"
      },
      "source": [
        "# Importing library \n",
        "import math \n",
        "import random \n",
        "import csv \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "filename = 'diabetes_2.csv'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XYkI9C717pH"
      },
      "source": [
        "result = []\n",
        "\n",
        "#utility functions\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(cm.shape[1])\n",
        "    plt.xticks(tick_marks, rotation=45)\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticklabels((ax.get_xticks() +1).astype(str))\n",
        "    plt.yticks(tick_marks)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], '.1f'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "def get_classification_report(y_test, y_pred):\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    df_classification_report = pd.DataFrame(report).iloc[:,:2]\n",
        "    return df_classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM_p06xb0Vo3"
      },
      "source": [
        "## Algorithms from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WXAdfJI0fdT"
      },
      "source": [
        "### Decision Tree Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJUwF0520e1C"
      },
      "source": [
        "def implement_decision_tree():\n",
        "\t\t# Load a CSV file\n",
        "\t\tdef load_csv(filename):\n",
        "\t\t\tfile = open(filename, \"rt\")\n",
        "\t\t\tlines = reader(file)\n",
        "\t\t\tdataset = list(lines)\n",
        "\t\t\treturn dataset\n",
        "\t\t\n",
        "\t\t# Split a dataset into k folds\n",
        "\t\tdef cross_validation_split(dataset, n_folds):\n",
        "\t\t\tdataset_split = list()\n",
        "\t\t\tdataset_copy = list(dataset)\n",
        "\t\t\tfold_size = int(len(dataset) / n_folds)\n",
        "\t\t\tfor i in range(n_folds):\n",
        "\t\t\t\tfold = list()\n",
        "\t\t\t\twhile len(fold) < fold_size:\n",
        "\t\t\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\t\t\tdataset_split.append(fold)\n",
        "\t\t\treturn dataset_split\n",
        "\t\t\n",
        "\t\t# Calculate accuracy percentage\n",
        "\t\tdef accuracy_metric(actual, predicted):\n",
        "\t\t\tcorrect = 0\n",
        "\t\t\tfor i in range(len(actual)):\n",
        "\t\t\t\tif actual[i] == predicted[i]:\n",
        "\t\t\t\t\tcorrect += 1\n",
        "\t\t\treturn correct / float(len(actual)) * 100.0\n",
        "\t\t\n",
        "\t\t# Evaluate an algorithm using a cross validation split\n",
        "\t\tdef evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\t\t\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\t\t\tscores = list()\n",
        "\t\t\tfor fold in folds:\n",
        "\t\t\t\ttrain_set = list(folds)\n",
        "\t\t\t\ttrain_set.remove(fold)\n",
        "\t\t\t\ttrain_set = sum(train_set, [])\n",
        "\t\t\t\ttest_set = list()\n",
        "\t\t\t\tfor row in fold:\n",
        "\t\t\t\t\trow_copy = list(row)\n",
        "\t\t\t\t\ttest_set.append(row_copy)\n",
        "\t\t\t\t\trow_copy[-1] = None\n",
        "\t\t\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\t\t\tactual = [row[-1] for row in fold]\n",
        "\t\t\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\t\t\tscores.append(accuracy)\n",
        "\t\t\treturn scores,predicted,actual\n",
        "\t\t\n",
        "\t\t# Split a dataset based on an attribute and an attribute value\n",
        "\t\tdef test_split(index, value, dataset):\n",
        "\t\t\tleft, right = list(), list()\n",
        "\t\t\tfor row in dataset:\n",
        "\t\t\t\tif row[index] < value:\n",
        "\t\t\t\t\tleft.append(row)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tright.append(row)\n",
        "\t\t\treturn left, right\n",
        "\t\t\n",
        "\t\t# Calculate the Gini index for a split dataset\n",
        "\t\tdef gini_index(groups, classes):\n",
        "\t\t\t# count all samples at split point\n",
        "\t\t\tn_instances = float(sum([len(group) for group in groups]))\n",
        "\t\t\t# sum weighted Gini index for each group\n",
        "\t\t\tgini = 0.0\n",
        "\t\t\tfor group in groups:\n",
        "\t\t\t\tsize = float(len(group))\n",
        "\t\t\t\t# avoid divide by zero\n",
        "\t\t\t\tif size == 0:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tscore = 0.0\n",
        "\t\t\t\t# score the group based on the score for each class\n",
        "\t\t\t\tfor class_val in classes:\n",
        "\t\t\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
        "\t\t\t\t\tscore += p * p\n",
        "\t\t\t\t# weight the group score by its relative size\n",
        "\t\t\t\tgini += (1.0 - score) * (size / n_instances)\n",
        "\t\t\treturn gini\n",
        "\t\t\n",
        "\t\t# Select the best split point for a dataset\n",
        "\t\tdef get_split(dataset):\n",
        "\t\t\tclass_values = list(set(row[-1] for row in dataset))\n",
        "\t\t\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "\t\t\tfor index in range(len(dataset[0])-1):\n",
        "\t\t\t\tfor row in dataset:\n",
        "\t\t\t\t\tgroups = test_split(index, row[index], dataset)\n",
        "\t\t\t\t\tgini = gini_index(groups, class_values)\n",
        "\t\t\t\t\tif gini < b_score:\n",
        "\t\t\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "\t\t\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\t\t\n",
        "\t\t# Create a terminal node value\n",
        "\t\tdef to_terminal(group):\n",
        "\t\t\toutcomes = [row[-1] for row in group]\n",
        "\t\t\treturn max(set(outcomes), key=outcomes.count)\n",
        "\t\t\n",
        "\t\t# Create child splits for a node or make terminal\n",
        "\t\tdef split(node, max_depth, min_size, depth):\n",
        "\t\t\tleft, right = node['groups']\n",
        "\t\t\tdel(node['groups'])\n",
        "\t\t\t# check for a no split\n",
        "\t\t\tif not left or not right:\n",
        "\t\t\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
        "\t\t\t\treturn\n",
        "\t\t\t# check for max depth\n",
        "\t\t\tif depth >= max_depth:\n",
        "\t\t\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "\t\t\t\treturn\n",
        "\t\t\t# process left child\n",
        "\t\t\tif len(left) <= min_size:\n",
        "\t\t\t\tnode['left'] = to_terminal(left)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnode['left'] = get_split(left)\n",
        "\t\t\t\tsplit(node['left'], max_depth, min_size, depth+1)\n",
        "\t\t\t# process right child\n",
        "\t\t\tif len(right) <= min_size:\n",
        "\t\t\t\tnode['right'] = to_terminal(right)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnode['right'] = get_split(right)\n",
        "\t\t\t\tsplit(node['right'], max_depth, min_size, depth+1)\n",
        "\t\t\n",
        "\t\t# Build a decision tree\n",
        "\t\tdef build_tree(train, max_depth, min_size):\n",
        "\t\t\troot = get_split(train)\n",
        "\t\t\tsplit(root, max_depth, min_size, 1)\n",
        "\t\t\treturn root\n",
        "\t\t\n",
        "\t\t# Make a prediction with a decision tree\n",
        "\t\tdef predict(node, row):\n",
        "\t\t\tif row[node['index']] < node['value']:\n",
        "\t\t\t\tif isinstance(node['left'], dict):\n",
        "\t\t\t\t\treturn predict(node['left'], row)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\treturn node['left']\n",
        "\t\t\telse:\n",
        "\t\t\t\tif isinstance(node['right'], dict):\n",
        "\t\t\t\t\treturn predict(node['right'], row)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\treturn node['right']\n",
        "\t\t\n",
        "\t\t# Classification and Regression Tree Algorithm\n",
        "\t\tdef decision_tree(train, test, max_depth, min_size):\n",
        "\t\t\ttree = build_tree(train, max_depth, min_size)\n",
        "\t\t\tpredictions = list()\n",
        "\t\t\tfor row in test:\n",
        "\t\t\t\tprediction = predict(tree, row)\n",
        "\t\t\t\tpredictions.append(prediction)\n",
        "\t\t\treturn(predictions)\n",
        "\t\t\n",
        "\n",
        "\t\t# evaluate algorithm\n",
        "\t\t#def implement_decision_tree():\n",
        "\t\tseed(1)\n",
        "\t\tdataset = load_csv(filename)\n",
        "\t\tn_folds = 5\n",
        "\t\tmax_depth = 5\n",
        "\t\tmin_size = 10\n",
        "\t\tscores,predictions ,test_label = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
        "\t\tprint('Scores: %s' % scores)\n",
        "\t\taccuracy=sum(scores)/float(len(scores))\n",
        "\t\tresult.append(accuracy)\n",
        "\t\tprint('Mean Accuracy: %.3f%%' % (accuracy))\n",
        "\n",
        "\t\tcm = confusion_matrix(test_label, predictions)\n",
        "\t\tnp.set_printoptions(precision=1) \n",
        "\t\tfig, ax = plt.subplots()\n",
        "\t\tplot_confusion_matrix(cm)\n",
        "\t\tplt.show()\n",
        "\t\t#classification report\n",
        "\t\tprint(\"\\nClassification Report:\\n\")\n",
        "\t\tprint(get_classification_report(test_label, predictions))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGAGvbFE5Q5"
      },
      "source": [
        "### Naive Bayes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLMi2EnuFRrZ"
      },
      "source": [
        "def implement_naive_bayes():\n",
        "    def encode_class(mydata): \n",
        "        classes = [] \n",
        "        for i in range(len(mydata)): \n",
        "            if mydata[i][-1] not in classes: \n",
        "                classes.append(mydata[i][-1]) \n",
        "        for i in range(len(classes)): \n",
        "            for j in range(len(mydata)): \n",
        "                if mydata[j][-1] == classes[i]: \n",
        "                    mydata[j][-1] = i \n",
        "        return mydata             \n",
        "                  \n",
        "      \n",
        "    # Splitting the data \n",
        "    def splitting(mydata, ratio): \n",
        "        train_num = int(len(mydata) * ratio) \n",
        "        train = [] \n",
        "        # initally testset will have all the dataset  \n",
        "        test = list(mydata) \n",
        "        while len(train) < train_num: \n",
        "            # index generated randomly from range 0  \n",
        "            # to length of testset \n",
        "            index = random.randrange(len(test)) \n",
        "            # from testset, pop data rows and put it in train \n",
        "            train.append(test.pop(index)) \n",
        "        return train, test \n",
        "      \n",
        "      \n",
        "    # Group the data rows under each class yes or  \n",
        "    # no in dictionary eg: dict[yes] and dict[no]  \n",
        "    def groupUnderClass(mydata): \n",
        "          dict = {} \n",
        "          for i in range(len(mydata)): \n",
        "              if (mydata[i][-1] not in dict): \n",
        "                  dict[mydata[i][-1]] = [] \n",
        "              dict[mydata[i][-1]].append(mydata[i]) \n",
        "          return dict\n",
        "      \n",
        "      \n",
        "    # Calculating Mean \n",
        "    def mean(numbers): \n",
        "        return sum(numbers) / float(len(numbers)) \n",
        "      \n",
        "    # Calculating Standard Deviation \n",
        "    def std_dev(numbers): \n",
        "        avg = mean(numbers) \n",
        "        variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers) - 1) \n",
        "        return math.sqrt(variance) \n",
        "      \n",
        "    def MeanAndStdDev(mydata): \n",
        "        info = [(mean(attribute), std_dev(attribute)) for attribute in zip(*mydata)] \n",
        "        del info[-1] \n",
        "        return info \n",
        "      \n",
        "    # find Mean and Standard Deviation under each class \n",
        "    def MeanAndStdDevForClass(mydata): \n",
        "        info = {} \n",
        "        dict = groupUnderClass(mydata) \n",
        "        for classValue, instances in dict.items(): \n",
        "            info[classValue] = MeanAndStdDev(instances) \n",
        "        return info \n",
        "      \n",
        "      \n",
        "    # Calculate Gaussian Probability Density Function \n",
        "    def calculateGaussianProbability(x, mean, stdev): \n",
        "        expo = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2)))) \n",
        "        return (1 / (math.sqrt(2 * math.pi) * stdev)) * expo \n",
        "      \n",
        "      \n",
        "    # Calculate Class Probabilities \n",
        "    def calculateClassProbabilities(info, test): \n",
        "        probabilities = {} \n",
        "        for classValue, classSummaries in info.items(): \n",
        "            probabilities[classValue] = 1\n",
        "            for i in range(len(classSummaries)): \n",
        "                mean, std_dev = classSummaries[i] \n",
        "                x = test[i] \n",
        "                probabilities[classValue] *= calculateGaussianProbability(x, mean, std_dev) \n",
        "        return probabilities \n",
        "      \n",
        "      \n",
        "    # Make prediction - highest probability is the prediction \n",
        "    def predict(info, test): \n",
        "        probabilities = calculateClassProbabilities(info, test) \n",
        "        bestLabel, bestProb = None, -1\n",
        "        for classValue, probability in probabilities.items(): \n",
        "            if bestLabel is None or probability > bestProb: \n",
        "                bestProb = probability \n",
        "                bestLabel = classValue \n",
        "        return bestLabel \n",
        "      \n",
        "      \n",
        "    # returns predictions for a set of examples \n",
        "    def getPredictions(info, test): \n",
        "        predictions = [] \n",
        "        for i in range(len(test)): \n",
        "            result = predict(info, test[i]) \n",
        "            predictions.append(result) \n",
        "        return predictions \n",
        "      \n",
        "    # Accuracy score \n",
        "    def accuracy_rate(test, predictions): \n",
        "        correct = 0\n",
        "        for i in range(len(test)): \n",
        "            if test[i][-1] == predictions[i]: \n",
        "                correct += 1\n",
        "        return (correct / float(len(test))) * 100.0\n",
        "      \n",
        "\n",
        "#def implement_naive_bayes():  \n",
        "    mydata = csv.reader(open(filename, \"rt\")) \n",
        "    mydata = list(mydata) \n",
        "    mydata = encode_class(mydata) \n",
        "    for i in range(len(mydata)): \n",
        "        mydata[i] = [float(x) for x in mydata[i]] \n",
        "      \n",
        "    ratio = 0.7\n",
        "    train_data, test_data = splitting(mydata, ratio) \n",
        "    print('Total number of examples are: ', len(mydata)) \n",
        "    print('Out of these, training examples are: ', len(train_data)) \n",
        "    print(\"Test examples are: \", len(test_data)) \n",
        "      \n",
        "    # prepare model \n",
        "    info = MeanAndStdDevForClass(train_data) \n",
        "      \n",
        "    # test model \n",
        "    predictions = getPredictions(info, test_data) \n",
        "    accuracy = accuracy_rate(test_data, predictions) \n",
        "    result.append(accuracy)\n",
        "    print(\"Accuracy of your model is: \", accuracy)\n",
        "\n",
        "    test_label=[i[-1] for i in test_data]\n",
        "    cm = confusion_matrix(test_label, predictions)\n",
        "    np.set_printoptions(precision=1) \n",
        "    # print('Confusion matrix')\n",
        "    # print(cm)\n",
        "    fig, ax = plt.subplots()\n",
        "    plot_confusion_matrix(cm)\n",
        "    plt.show()\n",
        "\n",
        "    #classification report\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    print(get_classification_report(test_label, predictions))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWb90_i7F1HX"
      },
      "source": [
        "### Support Vector Machine\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPZ_aOFOGSd4"
      },
      "source": [
        "def implement_SVM():\n",
        "    def compute_cost(W, X, Y):\n",
        "        # calculate hinge loss\n",
        "        N = X.shape[0]\n",
        "        distances = 1 - Y * (np.dot(X, W))\n",
        "        distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "        hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
        "\n",
        "        # calculate cost\n",
        "        cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
        "        return cost\n",
        "\n",
        "\n",
        "    def calculate_cost_gradient(W, X_batch, Y_batch):\n",
        "        if type(Y_batch) == np.float64:\n",
        "            Y_batch = np.array([Y_batch])\n",
        "            X_batch = np.array([X_batch])  \n",
        "\n",
        "        distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
        "        dw = np.zeros(len(W))\n",
        "\n",
        "        for ind, d in enumerate(distance):\n",
        "            if max(0, d) == 0:\n",
        "                di = W\n",
        "            else:\n",
        "                di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n",
        "            dw += di\n",
        "\n",
        "        dw = dw/len(Y_batch)  \n",
        "        return dw\n",
        "\n",
        "\n",
        "    def sgd(features, outputs):\n",
        "        max_epochs = 5000\n",
        "        weights = np.zeros(features.shape[1])\n",
        "        nth = 0\n",
        "        prev_cost = float(\"inf\")\n",
        "        cost_threshold = 0.01  \n",
        "        for epoch in range(1, max_epochs):\n",
        "            X, Y = shuffle(features, outputs)\n",
        "            for ind, x in enumerate(X):\n",
        "                ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "                weights = weights - (learning_rate * ascent)\n",
        "            if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
        "                cost = compute_cost(weights, features, outputs)\n",
        "    #             print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
        "                # stoppage criterion\n",
        "                if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
        "                    return weights\n",
        "                prev_cost = cost\n",
        "                nth += 1\n",
        "        return weights\n",
        "\n",
        "    # Accuracy score \n",
        "    def accuracy_rate(test, predictions): \n",
        "        correct = 0\n",
        "        for i in range(len(test)): \n",
        "            if test[i][-1] == predictions[i]: \n",
        "                correct += 1\n",
        "        return (correct / float(len(test))) * 100.0\n",
        "\n",
        "\n",
        "    def init(dataset):\n",
        "        Y = dataset.iloc[:,-1:]\n",
        "        X = dataset.iloc[:,:-1]\n",
        "\n",
        "        X_normalized = MinMaxScaler().fit_transform(X.values)\n",
        "        X = pd.DataFrame(X_normalized)\n",
        "\n",
        "        # insert 1 in every row for intercept b\n",
        "        X.insert(loc=len(X.columns), column='intercept', value=1)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.3, random_state=1)\n",
        "\n",
        "        W = sgd(X_train.values, y_train.values)\n",
        "        y_train_predicted = np.array([])\n",
        "        for i in range(X_train.shape[0]):\n",
        "            yp = np.sign(np.dot(X_train.values[i], W))\n",
        "            y_train_predicted = np.append(y_train_predicted, yp)\n",
        "\n",
        "        y_test_predicted = np.array([])\n",
        "        for i in range(X_test.shape[0]):\n",
        "            yp = np.sign(np.dot(X_test.values[i], W))\n",
        "            y_test_predicted = np.append(y_test_predicted, yp)\n",
        "        \n",
        "        print('Total number of examples are: ', len(dataset)) \n",
        "        print('Out of these, training examples are: ', len(X_train)) \n",
        "        print(\"Test examples are: \", len(X_test)) \n",
        "        accuracy = accuracy_score(y_test, y_test_predicted) * 100\n",
        "        result.append(accuracy)\n",
        "        print(\"Accuracy of your model is: \", accuracy)\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_test_predicted)\n",
        "        np.set_printoptions(precision=1) \n",
        "        fig, ax = plt.subplots()\n",
        "        plot_confusion_matrix(cm)\n",
        "        plt.show()\n",
        "\n",
        "        #classification report\n",
        "        print(\"\\nClassification Report:\\n\")\n",
        "        print(get_classification_report(y_test, y_test_predicted))\n",
        "\n",
        "#def implement_SVM():\n",
        "    # set hyper-parameters and call init\n",
        "    regularization_strength = 10000\n",
        "    learning_rate = 0.000001\n",
        "    dataset = pd.read_csv(filename)\n",
        "    init(dataset)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Pnjn_CL6w_"
      },
      "source": [
        "## Algorithms using built-in libraries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nq-LZxL3svS"
      },
      "source": [
        "### Naive Bayes using Scikit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhntGrOAFkgz"
      },
      "source": [
        "def implement_NaiveBayes_scikit():\n",
        "    dataset=pd.read_csv(filename,header=None)\n",
        "\n",
        "\n",
        "    X = dataset.loc[:,0:7]\n",
        "    y =dataset[8]\n",
        "      \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n",
        "\n",
        "      \n",
        "    # training the model on training set \n",
        "    from sklearn.naive_bayes import GaussianNB \n",
        "    gnb = GaussianNB() \n",
        "    gnb.fit(X_train, y_train) \n",
        "      \n",
        "    # making predictions on the testing set \n",
        "    y_pred = gnb.predict(X_test) \n",
        "      \n",
        "    # comparing actual response values (y_test) with predicted response values (y_pred) \n",
        "    from sklearn import metrics \n",
        "\n",
        "    print('Total number of examples are: ', len(dataset)) \n",
        "    print('Out of these, training examples are: ', len(X_train)) \n",
        "    print(\"Test examples are: \", len(X_test)) \n",
        "    accuracy= metrics.accuracy_score(y_test, y_pred)*100\n",
        "    result.append(accuracy)\n",
        "    print(\"Gaussian Naive Bayes model accuracy(in %):\",accuracy )\n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    np.set_printoptions(precision=1) \n",
        "    # print('Confusion matrix')\n",
        "    # print(cm)\n",
        "    fig, ax = plt.subplots()\n",
        "    plot_confusion_matrix(cm)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    #classification report\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    get_classification_report(y_test, y_pred)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6lzL69V38IS"
      },
      "source": [
        "### Support Vection machine using Scikit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os-2LvajSw6V"
      },
      "source": [
        "def implement_SVM_scikit():\n",
        "    model = SVC(kernel='linear')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    model = SVC(kernel='linear')\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = gnb.predict(X_test) \n",
        "      \n",
        "    # comparing actual response values (y_test) with predicted response values (y_pred) \n",
        "\n",
        "\n",
        "    print('Total number of examples are: ', len(dataset)) \n",
        "    print('Out of these, training examples are: ', len(X_train)) \n",
        "    print(\"Test examples are: \", len(X_test))\n",
        "    accuracy= metrics.accuracy_score(y_test, y_pred)*100\n",
        "    result.append(accuracy)\n",
        "    print(\" model accuracy(in %):\", accuracy)\n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    np.set_printoptions(precision=1) \n",
        "    # print('Confusion matrix')\n",
        "    # print(cm)\n",
        "    fig, ax = plt.subplots()\n",
        "    plot_confusion_matrix(cm)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    #classification report\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    get_classification_report(y_test, y_pred)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O27_R-dS43N"
      },
      "source": [
        "### Decision Tree using Scikit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_yM5tEVSxpD"
      },
      "source": [
        "def implement_Decision_tree_scikit():\n",
        "    tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "    tree.fit(X_train, y_train)\n",
        "\n",
        "    # making predictions on the testing set \n",
        "    y_pred = tree.predict(X_test) \n",
        "\n",
        "    from sklearn import metrics \n",
        "\n",
        "    print('Total number of examples are: ', len(dataset)) \n",
        "    print('Out of these, training examples are: ', len(X_train)) \n",
        "    print(\"Test examples are: \", len(X_test)) \n",
        "    accuracy=metrics.accuracy_score(y_test, y_pred)*100\n",
        "    result.append(accuracy)\n",
        "    print(\"Desicion tree model accuracy(in %):\", accuracy)\n",
        "\n",
        "\n",
        "    #confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    np.set_printoptions(precision=1) \n",
        "    # print('Confusion matrix')\n",
        "    # print(cm)\n",
        "    fig, ax = plt.subplots()\n",
        "    plot_confusion_matrix(cm)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    #classification report\n",
        "    print(\"\\nClassification Report:\\n\")\n",
        "    get_classification_report(y_test, y_pred)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fiTf_7k4u6-"
      },
      "source": [
        "## Comparing Algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj2YJC1VXfNm"
      },
      "source": [
        "def compare():\n",
        "    labels =['NB','SVM','DT']\n",
        "    acc_alg = [result[x] for x in range(len(result)) if x%2==0]\n",
        "    acc_lib = [result[x] for x in range(len(result)) if x%2!=0]\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.2  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,7))\n",
        "    fig.suptitle('Comparison of Algorithm ')\n",
        "    # plt.figure(figsize=(10,10))\n",
        "    rects1 = ax.bar(x - width/2, acc_alg, width, label='Algorithm from scratch')\n",
        "    rects2 = ax.bar(x + width/2, acc_lib, width, label='Algorithm using libraries')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Accuracy Score')\n",
        "    ax.set_xlabel('Classifiction Algorithms')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend(loc=1)\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvisoqBz5rD0"
      },
      "source": [
        "# Start Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqf2Nt4t751Q"
      },
      "source": [
        "## If using Anaconda\n",
        "### Please select the algorithm you want to implement from the choice list (input integer):\n",
        "#### 1. Decision Tree\n",
        "#### 2. Support Vector Machine\n",
        "#### 3. Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwjlltME79eb",
        "outputId": "62894f03-e25a-4793-ac86-94e1106e6574"
      },
      "source": [
        "choice = int(input('Your choice: '))\n",
        "filename = input('Please input dataset you want to use: ')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your choice: 2\n",
            "Please input dataset you want to use: diabetes_2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zlfbxUE9wAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "f675bcec-3780-4c48-ab2c-a469e730d7c9"
      },
      "source": [
        "if choice == 1:\n",
        "    implement_decision_tree()\n",
        "elif choice == 2:\n",
        "    implement_SVM()\n",
        "else:\n",
        "    implement_naive_bayes()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of examples are:  1999\n",
            "Out of these, training examples are:  1399\n",
            "Test examples are:  600\n",
            "Accuracy of your model is:  34.166666666666664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "               0           1\n",
            "precision    0.0    0.341667\n",
            "recall       0.0    1.000000\n",
            "f1-score     0.0    0.509317\n",
            "support    395.0  205.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9318an5HN2YH"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}